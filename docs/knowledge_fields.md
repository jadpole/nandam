# Knowledge Fields

The purpose of "fields" is to use LLMs to generate metadata from observations,
which can be used by tools (and other LLM agents) to apply filters and quickly
find relevant resources.


## Overview

The field generation system works as follows:

1. **Input**: A `BundleBody` containing observations (`$body`, `$chunk`, `$media`)
2. **Configuration**: Field definitions from `fields.yml` (or defaults)
3. **Processing**: LLM inference with structured JSON output
4. **Output**: `ResourceField` objects with generated metadata


## Supported Observations

Field generation is limited to observations that can be meaningfully interpreted
by LLMs:

| Type     | Description                                      |
|----------|--------------------------------------------------|
| `$body`  | The main document body (text or blob)            |
| `$chunk` | A section of a large document (e.g., `$chunk/00`)|
| `$media` | Embedded media files (images, diagrams, etc.)    |


## Structured Output

A single LLM request generates fields for multiple observations simultaneously.
The response uses structured JSON output where:

- Each key corresponds to a `(field_name, observation)` pair
- Every property is **nullable** (returns `null` when the field cannot be inferred)
- Property names are derived from observation URIs using `FieldName.try_normalize()`

### Property Naming Convention

Property names follow the pattern `{field_name}_{normalized_uri}`:

```
Observation URI:
  ndk://public/arxiv/2303.11366v2/$media/figures/reflexion_rl.pdf

Property name:
  description_publicarxiv230311366v2mediafiguresreflexionrlpdf
```

The normalization removes special characters (`/`, `$`, `.`, `-`) and converts
to lowercase to produce valid JSON property names.


## Token-Based Grouping

Observations are grouped into batches that fit within the LLM's context window:

- **Threshold**: 80,000 tokens per group (configurable via `THRESHOLD_SPLIT_GROUP`)
- **Token counting**: Uses `num_tokens()` method on each observation type
- **Batching**: Greedy algorithm that fills groups sequentially

Benefits of grouping:
1. **Efficiency**: Reduces the number of API calls
2. **Context**: LLM can use related observations to improve field quality
3. **Parallelism**: Groups can be processed independently

When observations span multiple groups, the `response_schema` only includes
properties for observations present in that specific request.


## Caching

Field values are cached to avoid redundant LLM calls:

- Previously generated fields are passed as `cached` to `generate_standard_fields()`
- The system checks `(field_name, observation_uri)` tuples against the cache
- Cached fields are **not** re-generated, even if other fields are requested

This enables incremental field generation when new observations are added to
a resource.


## Configuration

Fields are configured via `fields.yml` or fall back to built-in defaults.

### FieldConfig Schema

```yaml
fields:
  - name: description
    forall: [body, chunk, media]  # Which observation types to target
    prefixes: null                 # Optional: only target specific URI prefixes
    description: |
      Generate a concise, dense description...
```

| Property      | Type                        | Description                              |
|---------------|-----------------------------|------------------------------------------|
| `name`        | `FieldName`                 | Unique identifier for the field          |
| `forall`      | `["body", "chunk", "media"]`| Which observation types to generate for  |
| `prefixes`    | `list[str] \| null`         | URI prefixes to filter (null = all)      |
| `description` | `str`                       | Prompt instructions for the LLM          |


### Default Fields

When no `fields.yml` is present, two fields are generated by default:

#### `description`
- **Targets**: `$body`, `$chunk`, `$media`
- **Purpose**: A 2-3 sentence summary (≤50 words) that helps users and tools
  decide whether to consult the source for a given question.

#### `placeholder`
- **Targets**: `$media` only
- **Purpose**: A detailed textual representation of media content for AI agents
  that cannot view images natively (e.g., a MermaidJS diagram for a whiteboard
  photo).


## Implementation Details

### Entry Point

```python
async def generate_standard_fields(
    context: KnowledgeContext,
    cached: list[ResourceField],
    bundle: BundleBody,
) -> list[ResourceField]
```

### Processing Flow

```
BundleBody
    │
    ▼
observations() ──► [ObsBody, ObsChunk, ObsMedia, ...]
    │
    ▼
_explode_standard_fields() ──► [GenerateFieldsItem, ...]
    │                              (excludes cached fields)
    ▼
_group_observations_by_tokens() ──► [[obs1, obs2], [obs3], ...]
    │
    ▼
For each group:
    │
    ├─► _build_inference_params() ──► (system, schema, mapping)
    │
    ├─► _render_prompt() ──► [str | ContentBlob, ...]
    │
    ├─► inference.completion_json() ──► JSON string
    │
    └─► _parse_response() ──► [InferredField, ...]
    │
    ▼
[ResourceField, ...]
```

### Media Support

The prompt renderer supports inline images for multimodal LLMs:

- **Supported formats**: PNG, JPEG, WebP, HEIC, HEIF
- **Limit**: 20 media items per request (Gemini API limit)
- **Fallback**: Unsupported media uses placeholder text
